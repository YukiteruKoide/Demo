{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Bronze層テーブル作成（CSV to Delta）\n",
    "\n",
    "このノートブックでは、メダリオンアーキテクチャのBronze層を構築します。生のCSVファイルを読み込み、Deltaテーブル形式で保存します。\n",
    "\n",
    "## 目的\n",
    "- **データレイク基盤の構築**: 生データをそのまま保存\n",
    "- **データの永続化**: CSVファイルをDeltaテーブルに変換\n",
    "- **スキーマ推論**: データ型の自動判定と適用\n",
    "\n",
    "## 処理対象\n",
    "1. **transactions.csv** - 取引履歴データ\n",
    "2. **customers.csv** - 顧客マスタデータ\n",
    "3. **products.csv** - 商品マスタデータ\n",
    "4. **stores.csv** - 店舗マスタデータ\n",
    "\n",
    "## Bronze層の特徴\n",
    "- 生データをそのまま保存（最小限の変換のみ）\n",
    "- データの歴史と系譜を保持\n",
    "- 後続のSilver/Gold層の元データとして機能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 環境設定とカタログ/スキーマの切り替え\n",
    "\n",
    "処理前に使用するカタログとスキーマを定義し、明示的に切り替えます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42cf87fe-7c9b-4a75-9340-4396f306df96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 環境変数の定義（環境に応じて変更）\n",
    "catalog_name = \"users\"                      # Unity Catalogのカタログ名\n",
    "schema_name = \"yukiteru_koide\"              # スキーマ名\n",
    "base_path = \"/Volumes/users/yukiteru_koide/yukiterumart_etl\"  # CSVファイルが格納されたVolumeパス\n",
    "\n",
    "# CATALOGとSCHEMAを明示的に切り替え\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")    # カタログを指定\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")      # スキーマを指定\n",
    "\n",
    "print(f\"カタログ: {catalog_name}, スキーマ: {schema_name} に切り替えました\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## CSVファイルの読み込みとBronze層Deltaテーブル作成\n",
    "\n",
    "各CSVファイルを読み込み、スキーマを自動推論してDeltaテーブルとして保存します。\n",
    "\n",
    "### 処理の流れ\n",
    "1. CSVファイルを読み込み（ヘッダー付き、スキーマ自動推論）\n",
    "2. Deltaフォーマットで保存（上書きモード）\n",
    "3. Unity Catalogのテーブルとして登録\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1327c216-be77-4c6f-b2f4-c2660ecdad87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. トランザクションデータの処理\n",
    "print(\"📊 トランザクションデータを処理中...\")\n",
    "df_transactions = (\n",
    "    spark.read.format(\"csv\")                    # CSV形式で読み込み\n",
    "    .option(\"header\", True)                     # ヘッダー行あり\n",
    "    .option(\"inferSchema\", True)                # スキーマ自動推論\n",
    "    .load(f\"{base_path}/transactions.csv\")     # ファイルパス指定\n",
    ")\n",
    "# Deltaテーブルとして保存（既存テーブルは上書き）\n",
    "df_transactions.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"transactions\")\n",
    "print(f\"✅ transactionsテーブル作成完了 - レコード数: {df_transactions.count()}\")\n",
    "\n",
    "# 2. 顧客データの処理\n",
    "print(\"👥 顧客データを処理中...\")\n",
    "df_customers = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(f\"{base_path}/customers.csv\")\n",
    ")\n",
    "df_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"customers\")\n",
    "print(f\"✅ customersテーブル作成完了 - レコード数: {df_customers.count()}\")\n",
    "\n",
    "# 3. 商品データの処理\n",
    "print(\"🛍️ 商品データを処理中...\")\n",
    "df_products = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(f\"{base_path}/products.csv\")\n",
    ")\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products\")\n",
    "print(f\"✅ productsテーブル作成完了 - レコード数: {df_products.count()}\")\n",
    "\n",
    "# 4. 店舗データの処理\n",
    "print(\"🏪 店舗データを処理中...\")\n",
    "df_stores = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(f\"{base_path}/stores.csv\")\n",
    ")\n",
    "df_stores.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"stores\")\n",
    "print(f\"✅ storesテーブル作成完了 - レコード数: {df_stores.count()}\")\n",
    "\n",
    "print(\"\\n🎉 Bronze層テーブル作成が完了しました！\")\n",
    "\n",
    "# レビュー\n",
    "df_reviews = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(f\"{base_path}/reviews.csv\")\n",
    ")\n",
    "df_reviews.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"reviews\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_661d4af1-98fa-4a5d-a539-98ca8f2436c5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_csv_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
